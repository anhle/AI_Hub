{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_Prediction_with_scikit-learn_Census_Income_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7emZO/ZcmliiTAD6pwLp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhle/AI_Hub/blob/master/tutorial/Training_Prediction_with_scikit_learn_Census_Income_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpMRSD-wfLsu",
        "colab_type": "text"
      },
      "source": [
        "**Overview**\n",
        "\n",
        "This notebook uses the Census Income Data Set to demonstrate how to train a model and generate local predictions.\n",
        "\n",
        "The data\n",
        "The Census Income Data Set that this sample uses for training is provided by the UC Irvine Machine Learning Repository. Google has hosted the data on a public GCS bucket gs://cloud-samples-data/ml-engine/sklearn/census_data/ and also hosted in the UC Irvine dataset repository.\n",
        "\n",
        "Training file is adult.data\n",
        "Evaluation file is adult.test\n",
        "Note: Your typical development process with your own data would require you to upload your data to GCS so that you can access that data from inside your notebook. However, in this case, Google has put the data on GCS to avoid the steps of having you download the data from UC Irvine and then upload the data to GCS.\n",
        "\n",
        "\n",
        "\n",
        "**Build your model**\n",
        "\n",
        "First, you'll create the model (provided below). This is similar to your normal process for creating a scikit-learn model. However, there is one key difference:\n",
        "\n",
        "Downloading the data at the start of your file, so that you can access the data.\n",
        "The code in this file loads the data into a pandas DataFrame that can be used by scikit-learn. Then the model is fit against the training data. Lastly, sklearn's built in version of joblib is used to save the model to a file that can be uploaded to [ML Engine's prediction service](https://cloud.google.com/ml-engine/docs/scikit/getting-predictions#deploy_models_and_versions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy_nPlSGe2tA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelBinarizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JWZZZtvfugQ",
        "colab_type": "text"
      },
      "source": [
        "download the data (in this case, using the publicly hosted data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-xUoSlcfvDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the data\n",
        "! curl https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data --output adult.data\n",
        "! curl https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test --output adult.test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmt9Grvyf4hg",
        "colab_type": "text"
      },
      "source": [
        "Read in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj3A4yy6f5NF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the format of your input data including unused columns (These are the columns from the census data files)\n",
        "COLUMNS = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income-level'\n",
        "]\n",
        "\n",
        "# Categorical columns are columns that need to be turned into a numerical value to be used by scikit-learn\n",
        "CATEGORICAL_COLUMNS = ['workclass','education','marital-status','occupation','relationship','race','sex','native-country'\n",
        "]\n",
        "\n",
        "with open('./adult.data', 'r') as train_data:\n",
        "  raw_training_data = pd.read_csv(train_data, header=None, names=COLUMNS)\n",
        "\n",
        "# Remove the column you are trying to predict ('income-level') from our features list\n",
        "# Convert the Dataframe to a lists of lists\n",
        "train_features = raw_training_data.drop('income-level', axis=1).values.tolist()\n",
        "# Create our training labels list, convert the Dataframe to a lists of lists\n",
        "train_labels = (raw_training_data['income-level'] == ' >50K').values.tolist()\n",
        "\n",
        "with open('./adult.test', 'r') as test_data:\n",
        "  raw_testing_data = pd.read_csv(test_data, names=COLUMNS, skiprows=1)\n",
        "# Remove the column we are trying to predict ('income-level') from our features list\n",
        "# Convert the Dataframe to a lists of lists\n",
        "test_features = raw_testing_data.drop('income-level', axis=1).values.tolist()\n",
        "# Create our training labels list, convert the Dataframe to a lists of lists\n",
        "test_labels = (raw_testing_data['income-level'] == ' >50K.').values.tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EI9g7y7hCOM",
        "colab_type": "text"
      },
      "source": [
        "Since the census data set has categorical features, you need to convert them to numerical values. You'll use a list of pipelines to convert each categorical column and then use FeatureUnion to combine them before calling the RandomForestClassifier.\n",
        "\n",
        "Each categorical column needs to be extracted individually and converted to a numerical value. To do this, each categorical column will use a pipeline that extracts one feature column via SelectKBest(k=1) and a LabelBinarizer() to convert the categorical value to a numerical one. A scores array (created below) will select and extract the feature column. The scores array is created by iterating over the COLUMNS and checking if it is a CATEGORICAL_COLUMN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBWEkhQ1hHts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_pipelines = []\n",
        "\n",
        "for i, col in enumerate(COLUMNS[:-1]):\n",
        "  if col in CATEGORICAL_COLUMNS:\n",
        "    # Create a scores array to get the individual categorical column.\n",
        "    # Example:\n",
        "    #  data = [39, 'State-gov', 77516, 'Bachelors', 13, 'Never-married', 'Adm-clerical',\n",
        "    #         'Not-in-family', 'White', 'Male', 2174, 0, 40, 'United-States']\n",
        "    #  scores = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    #\n",
        "    # Returns: [['State-gov']]\n",
        "    # Build the scores array\n",
        "    scores = [0] * len(COLUMNS[:-1])\n",
        "    # This column is the categorical column you want to extract.\n",
        "    scores[i] = 1\n",
        "    skb = SelectKBest(k=1)\n",
        "    skb.scores_ = scores\n",
        "    # Convert the categorical column to a numerical value\n",
        "    lbn = LabelBinarizer()\n",
        "    r = skb.transform(train_features)\n",
        "    lbn.fit(r)\n",
        "    # Create the pipeline to extract the categorical feature\n",
        "    categorical_pipelines.append(\n",
        "      ('categorical-{}'.format(i), Pipeline([\n",
        "      ('SKB-{}'.format(i), skb),\n",
        "      ('LBN-{}'.format(i), lbn)])))\n",
        "\n",
        "# Create pipeline to extract the numerical features\n",
        "skb = SelectKBest(k=6)\n",
        "# From COLUMNS use the features that are numerical\n",
        "skb.scores_ = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
        "categorical_pipelines.append(('numerical', skb))\n",
        "\n",
        "# Combine all the features using FeatureUnion\n",
        "preprocess = FeatureUnion(categorical_pipelines)\n",
        "\n",
        "# Create the classifier\n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "# Transform the features and fit them to the classifier\n",
        "classifier.fit(preprocess.transform(train_features), train_labels)\n",
        "\n",
        "# Create the overall model as a single pipeline\n",
        "pipeline = Pipeline([\n",
        "  ('union', preprocess),\n",
        "  ('classifier', classifier)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ySn0_5viSSe",
        "colab_type": "text"
      },
      "source": [
        "Export the model to a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRwCliPwiSzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = 'model.joblib'\n",
        "joblib.dump(pipeline, model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvFUqaTHiXXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -al model.joblib\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4zhIw_9iaoS",
        "colab_type": "text"
      },
      "source": [
        "##Predictions\n",
        "Get one person that makes <=50K and one that makes >50K to test our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szW3qGK9igDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Show a person that makes <=50K:')\n",
        "print('\\tFeatures: {0} --> Label: {1}\\n'.format(test_features[0], test_labels[0]))\n",
        "\n",
        "with open('less_than_50K.json', 'w') as outfile:\n",
        "  json.dump(test_features[0], outfile)\n",
        "\n",
        "print('Show a person that makes >50K:')\n",
        "print('\\tFeatures: {0} --> Label: {1}'.format(test_features[3], test_labels[3]))\n",
        "\n",
        "with open('more_than_50K.json', 'w') as outfile:\n",
        "  json.dump(test_features[3], outfile)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZODCbTFoirLM",
        "colab_type": "text"
      },
      "source": [
        "##Use Python to make local predictions\n",
        "Test the model with the entire test set and print out some of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5WgAiLyivQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "local_results = pipeline.predict(test_features)\n",
        "local = pd.Series(local_results, name='local')\n",
        "local[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJOzDcBpi6AQ",
        "colab_type": "text"
      },
      "source": [
        "##Verify Results\n",
        "Use a confusion matrix to create a visualization of the local predicted results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UMHUdq9i8j5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual = pd.Series(test_labels, name='actual')\n",
        "local_predictions = pd.Series(local_results, name='local')\n",
        "\n",
        "pd.crosstab(actual, local_predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}